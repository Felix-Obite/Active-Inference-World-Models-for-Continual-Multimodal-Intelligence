## Self-Supervised Multimodal World Models for Continual Embodied Intelligence

This repository explores **Agentic AI** frameworks for integrating multimodal data using **advanced multilayer and graph-based representations**, grounded in **Continual Learning Variational Autoencoders (CL-VAEs)** and **Bayesian Active Inference (BAI)**.
The core objective is to develop adaptive, uncertainty-aware agents capable of lifelong learning, causal reasoning, and decision-making in complex and dynamic environments.

The framework unifies:
- Multimodal generative representation learning (e.g., graphs, sequences, symbolic latents),
- Bayesian belief updating via variational inference, and
- Active inferenceâ€“driven planning and control over learned latent world models.

By combining continual generative learning with agentic inference and counterfactual reasoning, the repository aims to support intelligent systems that can integrate new information over time, retain prior knowledge, and generalize beyond their training distributions.

Application Domains:

## ðŸ¤– Autonomous & Embodied Agents
The same principles extend to embodied intelligence, enabling agents to:
Continuously adapt to new environments and tasks,
Retain and reuse previously acquired strategies,
Actively explore uncertain regions to improve learning, planning, and robustness.

## ðŸ§  Clinical & Rare Disease Research
This framework enables AI agents to:
Integrate multimodal biomedical data (genomics, phenotypes, clinical records, knowledge graphs),
Learn graph-structured latent disease representations that evolve with experience,
Perform counterfactual reasoning and uncertainty-aware decision support,
Support interpretable, causal, and data-efficient inference in low-sample rare disease settings.

## Key Research Themes
Agentic AI & Active Inference
Multimodal and Graph-Based Generative Models
Continual & Lifelong Learning
Knowledge-Graphâ€“Conditioned World Models
Counterfactual Planning and Explainable Decision-Making


